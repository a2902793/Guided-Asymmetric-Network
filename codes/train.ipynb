{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path, math, argparse, random, logging, torch\n",
    "from timeit import default_timer as timer\n",
    "import options.options as option\n",
    "from utils import util\n",
    "from data import create_dataloader, create_dataset\n",
    "from models import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-opt', type=str, required=False, default='options/train/pretrain.json', help='Path to option JSON file.')\n",
    "opt = option.parse(parser.parse_args().opt, is_train=True)\n",
    "opt = option.dict_to_nonedict(opt)  # Convert to NoneDict, which return None for missing key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train from scratch OR resume training\n",
    "'''\n",
    "TODO: Auto connect newest state in resume_state path\n",
    "'''\n",
    "if opt['path']['resume_state']:  # resuming training\n",
    "    resume_state = torch.load(opt['path']['resume_state'])\n",
    "else:  # training from scratch\n",
    "    resume_state = None\n",
    "    util.mkdir_and_rename(opt['path']['experiments_root'])  # rename old folder if exists\n",
    "    util.mkdirs((path for key, path in opt['path'].items() if not key == 'experiments_root'\n",
    "                    and 'pretrain_model' not in key and 'resume' not in key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config loggers. Before it, the log will not work\n",
    "util.setup_logger(None, opt['path']['log'], 'train', level=logging.DEBUG, screen=True)\n",
    "util.setup_logger('val', opt['path']['log'], 'val', level=logging.DEBUG)\n",
    "logger = logging.getLogger('base')\n",
    "\n",
    "if resume_state:\n",
    "    logger.debug('Resuming training from epoch: {}, iter: {}.'.format(\n",
    "        resume_state['epoch'], resume_state['iter']))\n",
    "    option.check_resume(opt)  # check resume options\n",
    "\n",
    "logger.debug(option.dict2str(opt))\n",
    "# tensorboard logger\n",
    "# if opt['use_tb_logger'] and 'debug' not in opt['name']:\n",
    "#     from tensorboardX import SummaryWriter\n",
    "#     tb_logger = SummaryWriter(log_dir='../tb_logger/' + opt['name'])\n",
    "\n",
    "# random seed\n",
    "seed = opt['train']['manual_seed']\n",
    "if seed is None:\n",
    "    seed = random.randint(1, 10000)\n",
    "# logger.debug('Random seed: {}'.format(seed))\n",
    "util.set_random_seed(seed)\n",
    "\n",
    "torch.backends.cudnn.benckmark = True\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and val dataloader\n",
    "for phase, dataset_opt in opt['datasets'].items():\n",
    "    if phase == 'train':\n",
    "        train_set = create_dataset(dataset_opt)\n",
    "        train_size = int(math.ceil(len(train_set) / dataset_opt['batch_size']))\n",
    "        logger.debug('Number of train images: {:,d}, iters: {:,d}'.format(\n",
    "            len(train_set), train_size))\n",
    "        total_iters = int(opt['train']['niter'])\n",
    "        total_epochs = int(math.ceil(total_iters / train_size))\n",
    "        logger.debug('Total epochs needed: {:d} for iters {:,d}'.format(\n",
    "            total_epochs, total_iters))\n",
    "        train_loader = create_dataloader(train_set, dataset_opt)\n",
    "    elif phase == 'val':\n",
    "        val_set = create_dataset(dataset_opt)\n",
    "        val_loader = create_dataloader(val_set, dataset_opt)\n",
    "        logger.debug('Number of val images in [{:s}]: {:d}'.format(dataset_opt['name'],\n",
    "                                                                    len(val_set)))\n",
    "    else:\n",
    "        raise NotImplementedError('Phase [{:s}] is not recognized.'.format(phase))\n",
    "assert train_loader is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = create_model(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume training\n",
    "if resume_state:\n",
    "    start_epoch = resume_state['epoch']\n",
    "    current_step = resume_state['iter']\n",
    "    model.resume_training(resume_state)  # handle optimizers and schedulers\n",
    "else:\n",
    "    current_step = 0\n",
    "    start_epoch = 0\n",
    "\n",
    "logger.info('Start training from epoch: {:d}, iter: {:d}'.format(start_epoch, current_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.profiler.profile(\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler('./tb_logger'),\n",
    "    record_shapes=True,\n",
    "    with_stack=True\n",
    ") as prof:\n",
    "    for epoch in range(start_epoch, total_epochs):\n",
    "\n",
    "        for _, train_data in enumerate(train_loader):\n",
    "            current_step += 1\n",
    "            if current_step > total_iters:\n",
    "                break\n",
    "\n",
    "            # training\n",
    "            model.feed_data(train_data)\n",
    "            model.optimize_parameters(current_step)\n",
    "\n",
    "            # update learning rate\n",
    "            model.update_learning_rate()\n",
    "\n",
    "            # log\n",
    "            if current_step % opt['logger']['print_freq'] == 0:\n",
    "                logs = model.get_current_log()\n",
    "                message = '<epoch:{:3d}, iter:{:8,d}, lr:{:.3e}> '.format(\n",
    "                    epoch, current_step, model.get_current_learning_rate())\n",
    "                for k, v in logs.items():\n",
    "                    message += '{:s}: {:.4e} '.format(k, v)\n",
    "                    # tensorboard logger\n",
    "                    # if opt['use_tb_logger'] and 'debug' not in opt['name']:\n",
    "                    #     tb_logger.add_scalar(k, v, current_step)\n",
    "                logger.debug(message)\n",
    "\n",
    "            # validation\n",
    "            if current_step % opt['train']['val_freq'] == 0:\n",
    "                # start = timer()\n",
    "                avg_psnr_SRCNN = 0.0\n",
    "                avg_psnr_GAN = 0.0\n",
    "                avg_psnr = 0.0\n",
    "                idx = 0\n",
    "                for val_data in val_loader:\n",
    "                    idx += 1\n",
    "                    img_name = os.path.splitext(os.path.basename(val_data['LR_path'][0]))[0]\n",
    "                    img_dir = os.path.join(opt['path']['val_images'], img_name)\n",
    "                    util.mkdir(img_dir)\n",
    "\n",
    "                    model.feed_data(val_data)\n",
    "                    # model.feed_data2(val_data)\n",
    "                    model.test()\n",
    "\n",
    "                    visuals = model.get_current_visuals()\n",
    "                    SRCNN_img = util.tensor2img(visuals['fake_LF'])  # SRCNN做的\n",
    "                    GAN_img = util.tensor2img(visuals['fake_HF'])  # GAN做的\n",
    "                    sr_img = util.tensor2img(visuals['SR'])  # uint8\n",
    "                    gt_img = util.tensor2img(visuals['HR'])  # uint8\n",
    "\n",
    "                    # # Save SR images for reference\n",
    "                    # save_img_path = os.path.join(img_dir, '{:s}_{:d}.png'.format(\\\n",
    "                    #     img_name, current_step))\n",
    "                    \n",
    "                    # # util.save_img(SRCNN_img, os.path.join(img_dir, 'SRCNN_{:s}_{:d}.png'.format(img_name, current_step)))\n",
    "                    # # util.save_img(GAN_img, os.path.join(img_dir, 'GAN_{:s}_{:d}.png'.format(img_name, current_step)))\n",
    "                    # util.save_img(sr_img, save_img_path)\n",
    "\n",
    "                    # calculate PSNR\n",
    "                    crop_size = opt['scale']\n",
    "\n",
    "                    SRCNN_img = SRCNN_img / 255.\n",
    "                    GAN_img = GAN_img / 255.\n",
    "                    gt_img = gt_img / 255.\n",
    "                    sr_img = sr_img / 255.\n",
    "\n",
    "                    cropped_SRCNN_img = SRCNN_img[crop_size:-crop_size, crop_size:-crop_size, :]\n",
    "                    cropped_GAN_img = GAN_img[crop_size:-crop_size, crop_size:-crop_size, :]\n",
    "                    cropped_sr_img = sr_img[crop_size:-crop_size, crop_size:-crop_size, :]\n",
    "                    cropped_gt_img = gt_img[crop_size:-crop_size, crop_size:-crop_size, :]\n",
    "                    \n",
    "                    avg_psnr_SRCNN += util.calculate_psnr(cropped_SRCNN_img * 255, cropped_gt_img * 255)\n",
    "                    avg_psnr_GAN += util.calculate_psnr(cropped_GAN_img * 255, cropped_gt_img * 255)\n",
    "                    avg_psnr += util.calculate_psnr(cropped_sr_img * 255, cropped_gt_img * 255)\n",
    "                    \n",
    "                avg_psnr_SRCNN = avg_psnr_SRCNN / idx\n",
    "                avg_psnr_GAN = avg_psnr_GAN / idx \n",
    "                avg_psnr = avg_psnr / idx\n",
    "\n",
    "                # log\n",
    "                # logger.debug('# Validation # PSNR: {:.4e} PSNR_SRCNN: {:.4e} PSNR_GAN: {:.4e}'.format(avg_psnr, avg_psnr_SRCNN, avg_psnr_GAN))\n",
    "                logger.debug('# Validation # PSNR: {:.4e} '.format(avg_psnr))\n",
    "                logger_val = logging.getLogger('val')  # validation logger\n",
    "                logger_val.info('<epoch:{:3d}, iter:{:8,d}> psnr: {:.4e} psnr_SRCNN: {:.4e} psnr_GAN: {:.4e}'.format(\n",
    "                    epoch, current_step, avg_psnr, avg_psnr_SRCNN, avg_psnr_GAN))\n",
    "                # end = timer()\n",
    "                # logger.info(f'validation: {end - start} seconds')\n",
    "\n",
    "                # logger_val.info('<epoch:{:3d}, iter:{:8,d}> psnr: {:.4e}'.format(\n",
    "                #     epoch, current_step, avg_psnr))\n",
    "                # tensorboard logger\n",
    "                # if opt['use_tb_logger'] and 'debug' not in opt['name']:\n",
    "                #     tb_logger.add_scalar('psnr', avg_psnr, current_step)\n",
    "            \n",
    "            # save models and training states\n",
    "            if current_step % opt['logger']['save_checkpoint_freq'] == 0:\n",
    "                logger.debug('Saving models and training states.')\n",
    "                model.save(current_step)\n",
    "                model.save_training_state(epoch, current_step)\n",
    "            prof.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug('Saving the final model.')\n",
    "model.save('latest')\n",
    "logger.debug('End of training.')\n",
    "print(\"End of train.py\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
